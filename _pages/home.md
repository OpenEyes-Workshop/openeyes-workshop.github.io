---
layout: project
urltitle:  "OpenEyes 2020: Eye Gaze in AR, VR, and in the Wild"
title: "OpenEyes 2020: Eye Gaze in AR, VR, and in the Wild"
categories: iccv, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /
favicon: /static/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>OpenEyes: Eye Gaze in AR, VR, and in the Wild</h1></center>
    <center><h2>ECCV 2020 Workshop, Glasgow, Scotland</h2></center>
    <!--<center>Sunday October 27 2019, 8:30am - 12:50pm</center>-->
    <!--<center>Location: <b>Room 318 A</b></center>-->
  </div>
</div>

<!--
<hr>
<div class="row">
  <div class="col-md-12">
    <div class="alert alert-success">
      Please <b><a href="https://groups.google.com/forum/#!forum/gaze-in-the-wild" target="_blank">subscribe</a></b> to our
	  <b><a href="https://groups.google.com/forum/#!forum/gaze-in-the-wild" target="_blank">new mailing list</a></b>
	  to gain access to our speakers' slides and future updates!
    </div>
  </div>
</div>
-->

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
  <p>
  With the advent of consumer products, AR and VR as a form of immersive technology is gaining
  mainstream attention. However, immersive technology is still in its infancy, as both users and
  developers figure out the right recipe for the technology to garner mass appeal.
  </p>
  <p>
  Eye tracking, a technology that measures where an individual is looking and can enable
  inference of user attention, could be a key driver of mass appeal for the next generation of
  immersive technologies, provided user awareness and privacy related to eye-tracking features
  are taken into account. As such, there is a growing interest in improving the state-of-the-art
  for eye tracking technology. In the past three years, investigations into gaze estimation and
  prediction methods produced significant improvements in robustness and accuracy by adopting
  increasingly unique deep neural network architectures. These improvements allow innovative
  applications for this technology, such as zero-shot image classification and generalized human
  attention and intent estimation.
  </p>
  <p>
  Open forums of discussion provide opportunities to further improve eye tracking technology,
  especially in areas like scale and generalization challenges in the next generation of AR and
  VR systems. For that reason,
  <a target="_blank" href="https://research.fb.com/">Facebook</a> organized the first challenge
  <a target="_blank" href="https://research.fb.com/programs/the-2019-openeds-workshop-eye-tracking-for-vr-and-ar/">“Eye Tracking for VR and AR (OpenEDS)”</a>
  at the ICCV 2019 and the independent GAZE committee organized a workshop titled
  <a target="_blank" href="https://gazeworkshop.github.io/">“Gaze Estimation and Prediction in the Wild (GAZE)”</a>.
  </p>
  <p>
  For 2020, the Facebook and GAZE committees are partnering to host a joint workshop titled
  “Eye Gaze in VR, AR, and in the Wild” at the biennial ECCV conference. The workshop will host
  two tracks: the first focuses on gaze estimation and prediction methods, with a focus on
  accuracy and robustness in natural settings (in-the-wild); the second track focuses on the
  scale and generalization problem for eye tracking systems operating on AR and VR platforms.
  The second track also includes the 2020 eye tracking challenge. Stay tuned for more details
  about the new challenge.
  </p>

  <p>
  The following topics are of particular interest to the joint workshop:
  </p>
  <ul>
  <li>
  Proposal of novel eye detection, gaze estimation pipelines using deep neural networks that incorporate one or all of the following:
  <ul>
  <li>Geometric/anatomical constraints into the network in a differentiable manner.</li>
  <li>Demonstration of robustness to conditions where current methods fail (illumination, appearance, low-resolution etc.).</li>
  <li>Robust estimation from different data modalities such as RGB, depth, and near IR.</li>
  <li>Use of additional cues, such as task context, temporal data, eye movement classification.</li>
  </ul>
  </li>
  <li>Designing new, accurate metrics to account for rapid eye movements in the real world.</li>
  <li>Semi-/un-/self-supervised learning, meta-learning, domain adaptation, attention mechanisms and other related machine learning methods for gaze estimation.</li>
  <li>Methods for temporal gaze estimation and prediction including Bayesian methods.</li>
  <li>Unsupervised semantic segmentation of eye regions.</li>
  <li>Active learning frameworks for semantic segmentation of eye images.</li>
  <li>Generative models for eye image synthesis and gaze estimation.</li>
  <li>Transfer learning for eye tracking from simulation data to real data.</li>
  <li>Domain transfer applications for eye tracking.</li>
  </ul>

  <p>
  This workshop will accept submissions of both published and unpublished works. We will also
  solicit high-quality eye tracking-related papers rejected at ECCV 2020, accompanied by the
  reviews and a letter of changes which clearly states the changes made to address comments by
  the previous reviewers. Accepted papers may be featured as spotlight talks and posters.
  </p>
</div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>Friday, 5th June 2020</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>Friday, 3rd July 2020</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>Friday, 17th July 2020</td>
        </tr>
        <!--<tr id="schedule">
          <td>Extended Abstracts Deadline</td>
          <td>TBD </td>
        </tr>-->
        <tr>
          <td>Workshop Date</td>
          <td>TBD </td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Invited Keynote Speakers</h2>
    <p>TBD</p>
  </div>
</div>
<br>


<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <b>Track 1 (AM): Gaze Estimation and Prediction in the Wild</b>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "/static/img/people/hj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "/static/img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/zhang/">
      <img class="people-pic" src="{{ "/static/img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "/static/img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "/static/img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <b>Track 2 (PM): Eye Tracking for VR and AR</b>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/rc.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Robert Cavin
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/cp.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Cristina Palmero
      <h6>Universitat de Barcelona (UB)</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/jc.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Jixu Chen
      <h6>Facebook</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/af.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Alexander Fix
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/eg.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Elias Guestrin
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/ok.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Oleg Komogortsev
      <h6>Texas State University</h6>
    </div>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/kk.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Kapil Krishnakumar
      <h6>Facebook</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/as.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Abhishek Sharma
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/ys.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Yiru Shen
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/th.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Tarek Hefny
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/kb.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Karsten Behrendt
      <h6>Facebook</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <img class="people-pic" src="{{ "/static/img/people/st.jpg" | prepend:site.baseurl }}">
    <div class="people-name">
      Sachin S. Talathi
      <h6>Facebook Reality Labs</h6>
    </div>
  </div>
</div>
<br>

<div class="row" id="sponsors">
  <div class="col-xs-12">
    <h2>Workshop sponsored by:</h2>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-12">
    <b>Track 1 (AM): Gaze Estimation and Prediction in the Wild</b>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-2"></div>
  <!--<div class="col-xs-4 sponsor">
    <a href="https://www.samsung.com/" target="_blank"><img src="{{ "/static/img/samsung.jpg" | prepend:site.baseurl }}" /></a>
  </div>-->
  <!--<div class="col-xs-1"></div>-->
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/" target="_blank"><img src="{{ "/static/img/nvidia.jpg" | prepend:site.baseurl }}" /></a>
  </div>
  <!--<div class="col-xs-1"></div>-->
  <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/" target="_blank"><img src="{{ "/static/img/tobii.jpg" | prepend:site.baseurl }}" /></a>
  </div>
  <div class="col-xs-2"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-12">
    <b>Track 2 (PM): Eye Tracking for VR and AR</b>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-12 sponsor">
    <a href="https://research.fb.com/category/augmented-reality-virtual-reality/" target="_blank"><img width="400px" src="{{ "/static/img/frl.svg" | prepend:site.baseurl }}" /></a>
  </div>
</div>


<br>
<hr>
